{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x106750b10>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jeremiah/Studies/NLP/Ass2/BiLSTM\n"
     ]
    }
   ],
   "source": [
    "#  current directory\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Therefore,', 'while', 'interpreting', 'statutory', 'provisions,', 'the', 'courts', 'should', 'keep', 'in', 'mind', 'the', 'objectives', 'or', 'purpose', 'for', 'which', 'statute', 'has', 'been', 'enacted.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "with open('training_data.json', 'r') as file:\n",
    "    training_data = json.load(file)\n",
    "    # save training data into panda dataframe\n",
    "\n",
    "\n",
    "\n",
    "with open('test.json', 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "    # save test data into panda dataframe\n",
    "\n",
    "with open('validation.json', 'r') as file:\n",
    "    validation_data = json.load(file)\n",
    "    # save validation data into panda dataframe\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(training_data)\n",
    "df=df.T\n",
    "\n",
    "\n",
    "valdf = pd.DataFrame(validation_data)\n",
    "valdf=valdf.T\n",
    "\n",
    "testdf = pd.DataFrame(test_data)\n",
    "testdf=testdf.T\n",
    "\n",
    "\n",
    "texts = df['text'].tolist()\n",
    "labels = df['labels'].tolist()\n",
    "import re\n",
    "\n",
    "valtexts = valdf['text'].tolist()\n",
    "vallabels = valdf['labels'].tolist()\n",
    "\n",
    "testtexts = testdf['text'].tolist()\n",
    "testlabels = testdf['labels'].tolist()\n",
    "\n",
    "\n",
    "def convert_data(texts, labels):\n",
    "    converted_data = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        words = text.split()\n",
    "        words=[word for word in words if word!='']\n",
    "        tags = label\n",
    "        converted_data.append((words, tags))\n",
    "    return converted_data\n",
    "\n",
    "training_data = convert_data(texts, labels)\n",
    "print(training_data[0])\n",
    "validation_data = convert_data(valtexts, vallabels)\n",
    "\n",
    "test_data = convert_data(testtexts, testlabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B_PROVISION': 0, 'B_OTHER_PERSON': 1, 'B_JUDGE': 2, 'I_STATUTE': 3, 'B_CASE_NUMBER': 4, 'I_RESPONDENT': 5, 'B_WITNESS': 6, 'B_GPE': 7, 'I_WITNESS': 8, 'I_ORG': 9, 'O': 10, 'I_PROVISION': 11, 'I_COURT': 12, 'I_GPE': 13, 'I_OTHER_PERSON': 14, 'B_STATUTE': 15, 'I_DATE': 16, 'I_CASE_NUMBER': 17, 'B_ORG': 18, 'I_PRECEDENT': 19, 'B_RESPONDENT': 20, 'B_COURT': 21, 'B_DATE': 22, 'B_PRECEDENT': 23, 'B_PETITIONER': 24, 'I_PETITIONER': 25, 'I_JUDGE': 26}\n"
     ]
    }
   ],
   "source": [
    "tags_vals = list(set(df['labels'].sum()))\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import json\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Therefore,', 'while', 'interpreting', 'statutory', 'provisions,', 'the', 'courts', 'should', 'keep', 'in', 'mind', 'the', 'objectives', 'or', 'purpose', 'for', 'which', 'statute', 'has', 'been', 'enacted.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']) (['The', 'petitioner', 'in', 'W.P.No.15821', 'of', '2008', 'was', 'never', 'considered', 'for', 'appointment', 'under', 'the', 'National', 'Rural', 'Employment', 'Guarantee', 'Scheme', 'either', 'through', 'Employment', 'Exchange', 'sponsorship', 'or', 'by', 'Outsourcing', 'Agencies.'], ['O', 'O', 'O', 'B_CASE_NUMBER', 'I_CASE_NUMBER', 'I_CASE_NUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_ORG', 'I_ORG', 'I_ORG', 'I_ORG', 'I_ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0],training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "unk = \"UNK\"\n",
    "\n",
    "\n",
    "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "word_to_ix[unk] = len(word_to_ix)\n",
    "\n",
    "# tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "tag_to_ix= {tag: i for i, tag in enumerate(set([tag for sentence, tags in training_data for tag in tags]))   }\n",
    "tag_to_ix[START_TAG] = len(tag_to_ix)\n",
    "tag_to_ix[STOP_TAG] = len(tag_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    vocab_size = len(to_ix)\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        if w in to_ix:\n",
    "            idxs.append(to_ix[w])\n",
    "        else:\n",
    "            idxs.append(to_ix[unk])\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"   \n",
    "STOP_TAG = \"<STOP>\"\n",
    "# create a tag for unkwown data\n",
    "Unk_TAG=\"<UNK>\"\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, pretrained_embeddings=None):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.word_embeds = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "            print('-----Using pretrained embeddings-------')\n",
    "        else:\n",
    "            self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "    \n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec_embeddings(filepath):\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(filepath, binary=True)\n",
    "    return word_vectors\n",
    "\n",
    "def prepare_embedding_matrix(word_vectors, word_to_ix, embedding_dim=300):\n",
    "    matrix_len = len(word_to_ix)\n",
    "    weights_matrix = torch.zeros((matrix_len, embedding_dim))\n",
    "\n",
    "    for word, i in word_to_ix.items():\n",
    "        try: \n",
    "            weights_matrix[i] = torch.from_numpy(word_vectors[word])\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = torch.randn((embedding_dim,))  # Random initialization for words not in Word2Vec\n",
    "\n",
    "\n",
    "    return weights_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(predictions, targets, tag_to_ix):\n",
    "    # Convert predictions and targets to flat lists\n",
    "    predictions_flat = [p for seq in predictions for p in seq]\n",
    "    targets_flat = [t for seq in targets for t in seq]\n",
    "\n",
    "    # Convert tag indices to tags\n",
    "    predictions_tags = [list(tag_to_ix.keys())[list(tag_to_ix.values()).index(p)] for p in predictions_flat]\n",
    "    targets_tags = [list(tag_to_ix.keys())[list(tag_to_ix.values()).index(t)] for t in targets_flat]\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1_score = metrics.f1_score(targets_tags, predictions_tags, average='weighted')\n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec_file = 'GoogleNews-vectors-negative300.bin'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Using pretrained embeddings-------\n",
      "Epoch 1/1\n",
      "----------\n",
      "0% of data trained\n",
      "20% of data trained\n",
      "40% of data trained\n",
      "60% of data trained\n",
      "80% of data trained\n",
      "100% of data trained\n",
      "epoch 1/1\n",
      "Train loss: 20.1530 | Validation loss: 8.7987 | Validation F1 score: 0.2565\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 142\u001b[0m\n\u001b[1;32m    139\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Plot the validation F1 score\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_f1_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining F1 score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), val_f1_scores, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation F1 score\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    144\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "unk = \"<UNK>\"\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "\n",
    "\n",
    "word_vectors = load_word2vec_embeddings(word2vec_file)\n",
    "# embedding_matrix = prepare_embedding_matrix(word_vectors, word_to_ix)\n",
    "\n",
    "\n",
    "\n",
    "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "word_to_ix[unk] = len(word_to_ix)\n",
    "\n",
    "# tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "tag_to_ix= {tag: i for i, tag in enumerate(set([tag for sentence, tags in training_data for tag in tags]))   }\n",
    "tag_to_ix[START_TAG] = len(tag_to_ix)\n",
    "tag_to_ix[STOP_TAG] = len(tag_to_ix)\n",
    "\n",
    "\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = prepare_embedding_matrix(word_vectors, word_to_ix, embedding_dim=300)\n",
    "# Initialize and train the model with pre-trained embeddings\n",
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, pretrained_embeddings=embedding_matrix)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "# Check predictions before training\n",
    "# with torch.no_grad():\n",
    "#     precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "#     precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "#     print(model(precheck_sent))\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Lists to store losses and F1 scores\n",
    "train_losses = []\n",
    "train_f1_scores = []\n",
    "val_losses = []\n",
    "val_f1_scores = []\n",
    "num_epochs = 1\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # show live status of each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print('-' * 10)\n",
    "    total_train_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for sentence, tags in training_data:\n",
    "        # Clear the gradients before training\n",
    "        # print progress of each epoch after every 20% of data trainig\n",
    "        if (len(sentence)!=len(tags)):\n",
    "            print(sentence, tags)\n",
    "            break\n",
    "        if (training_data.index((sentence, tags)) % (len(training_data)//5) == 0):\n",
    "            print(f\"{(training_data.index((sentence, tags))/len(training_data))*100:.0f}% of data trained\")\n",
    "\n",
    "        model.zero_grad()\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "        \n",
    "        # Calculate the negative log likelihood loss\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    train_loss = total_train_loss / len(training_data)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss=0\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in validation_data:\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "        \n",
    "            predictions = model(sentence_in)\n",
    "            val_loss += model.neg_log_likelihood(sentence_in, targets).item()\n",
    "\n",
    "            # predictions_labels=[list(tag_to_ix.keys())[list(tag_to_ix.values()).index(p)] for p in predictions[1]]\n",
    "            predictions_labels=[]\n",
    "            for p in predictions[1]:\n",
    "                if p in tag_to_ix.values():\n",
    "                    predictions_labels.append(list(tag_to_ix.keys())[list(tag_to_ix.values()).index(p)])\n",
    "                else:\n",
    "                    predictions_labels.append('O')\n",
    "            all_predictions.extend(predictions_labels)\n",
    "            all_true_labels.extend(tags)\n",
    "\n",
    "        \n",
    "        # Calculate validation loss\n",
    "        val_loss =val_loss/len(validation_data)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate validation F1 score\n",
    "        val_f1_score = f1_score(all_true_labels, all_predictions, average='macro')\n",
    "        val_f1_scores.append(val_f1_score)\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    print(f\"epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train loss: {train_loss:.4f} | Validation loss: {val_loss:.4f} | Validation F1 score: {val_f1_score:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "#  plot the training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Training loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the validation F1 score\n",
    "plt.plot(range(1, num_epochs+1), train_f1_scores, label='Training F1 score')\n",
    "plt.plot(range(1, num_epochs+1), val_f1_scores, label='Validation F1 score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 score')\n",
    "plt.title('Training and Validation F1 score')\n",
    "plt.legend() \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "----------\n",
      "0% of data trained\n",
      "20% of data trained\n",
      "40% of data trained\n",
      "60% of data trained\n",
      "80% of data trained\n",
      "100% of data trained\n",
      "epoch 1/1\n",
      "Train loss: 15.0884 | Validation loss: 7.4762 | Validation F1 score: 0.3218\n",
      "\n",
      "Epoch 2/1\n",
      "----------\n",
      "0% of data trained\n",
      "20% of data trained\n",
      "40% of data trained\n",
      "60% of data trained\n",
      "80% of data trained\n",
      "100% of data trained\n",
      "epoch 2/1\n",
      "Train loss: 14.4889 | Validation loss: 7.6905 | Validation F1 score: 0.3238\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#  plot the training and validation loss\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), val_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     83\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (5,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.05, weight_decay=1e-4)\n",
    "# print(\"a\")\n",
    "for epoch in range(2):\n",
    "    # show live status of each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print('-' * 10)\n",
    "    total_train_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for sentence, tags in training_data:\n",
    "        # Clear the gradients before training\n",
    "        # print progress of each epoch after every 20% of data trainig\n",
    "        if (len(sentence)!=len(tags)):\n",
    "            print(sentence, tags)\n",
    "            break\n",
    "        if (training_data.index((sentence, tags)) % (len(training_data)//5) == 0):\n",
    "            print(f\"{(training_data.index((sentence, tags))/len(training_data))*100:.0f}% of data trained\")\n",
    "\n",
    "        model.zero_grad()\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "        \n",
    "        # Calculate the negative log likelihood loss\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    train_loss = total_train_loss / len(training_data)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss=0\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in validation_data:\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "        \n",
    "            predictions = model(sentence_in)\n",
    "            val_loss += model.neg_log_likelihood(sentence_in, targets).item()\n",
    "\n",
    "            # predictions_labels=[list(tag_to_ix.keys())[list(tag_to_ix.values()).index(p)] for p in predictions[1]]\n",
    "            predictions_labels=[]\n",
    "            for p in predictions[1]:\n",
    "                if p in tag_to_ix.values():\n",
    "                    predictions_labels.append(list(tag_to_ix.keys())[list(tag_to_ix.values()).index(p)])\n",
    "                else:\n",
    "                    predictions_labels.append('O')\n",
    "            all_predictions.extend(predictions_labels)\n",
    "            all_true_labels.extend(tags)\n",
    "\n",
    "        \n",
    "        # Calculate validation loss\n",
    "        val_loss =val_loss/len(validation_data)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate validation F1 score\n",
    "        val_f1_score = f1_score(all_true_labels, all_predictions, average='macro')\n",
    "        val_f1_scores.append(val_f1_score)\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    print(f\"epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train loss: {train_loss:.4f} | Validation loss: {val_loss:.4f} | Validation F1 score: {val_f1_score:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), 'model.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29991183269161636\n",
      "0.8517806160781367\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def evaluate_model(model, data, word_to_ix, tag ):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in data:\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "            predictions = model(sentence_in)\n",
    "            predictions_labels=[list(tag_to_ix.keys())[list(tag_to_ix.values()).index(p)] for p in predictions[1]]\n",
    "            all_predictions.extend(predictions_labels)\n",
    "            all_true_labels.extend(tags)\n",
    "            \n",
    "        print(f1_score(all_true_labels, all_predictions, average='macro'))\n",
    "        print(accuracy_score(all_true_labels, all_predictions))\n",
    "        # accuracy_score = accuracy_score(all_true_labels, all_predictions)\n",
    "        # return f1_scor\n",
    "        return 100\n",
    "    \n",
    "# Evaluate the model on the test set\n",
    "test_f1_score = evaluate_model(model, test_data, word_to_ix, tag_to_ix)\n",
    "# print(f\"Test F1 score: {test_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'f1_score' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining F1 score:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_to_ix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag_to_ix\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[27], line 17\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, data, word_to_ix, tag)\u001b[0m\n\u001b[1;32m     14\u001b[0m     all_true_labels\u001b[38;5;241m.\u001b[39mextend(tags)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate F1 score\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m f1_score \u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m(all_true_labels, all_predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m accuracy_score \u001b[38;5;241m=\u001b[39m accuracy_score(all_true_labels, all_predictions)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# return f1_scor\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'f1_score' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "print('Training F1 score:', evaluate_model(model, training_data, word_to_ix, tag_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
